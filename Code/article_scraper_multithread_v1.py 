# Scraping Articles using newspaper3k package: Multi-Threading Edition
# Documentation could be found here: https://newspaper.readthedocs.io/en/latest/

# Import Newspaper API that I will be using
import newspaper
from newspaper import Article
from newspaper import Source
from newspaper import news_pool

# Other important Plugins
import pandas as pd

#List of online sources we want to scrape
cnn = newspaper.build("https://edition.cnn.com/")
bbc = newspaper.build("https://www.bbc.com/news")
slate = newspaper.build('https://slate.com')
breitbart = newspaper.build("https://www.breitbart.com/politics/")
politico = newspaper.build("https://www.politico.com/")
thehill = newspaper.build("https://thehill.com/")
cbc = newspaper.build('https://www.cbc.ca/news')
washingtonpost = newspaper.build("https://www.washingtonpost.com/")
globeandmail = newspaper.build("https://www.theglobeandmail.com/")
tc = newspaper.build('https://techcrunch.com/')
gamespot = newspaper.build('https://www.gamespot.com/news/')
globalnewsca = newspaper.build('https://globalnews.ca/')
thestar = newspaper.build('https://www.thestar.com/')
cna = newspaper.build('https://www.channelnewsasia.com/news/international')

#Combine all the sources
list_of_sources = [cnn,bbc,slate,breitbart,politico,
                   thehill,cbc,washingtonpost,globeandmail,
                   tc,gamespot,globalnewsca,thestar,cna]

#Intaitate Muli-Threading Downloads
news_pool.set(list_of_sources, threads_per_source = 3) #3 thrads per each source
news_pool.join()

#Create our final dataframe
df_articles = pd.DataFrame()

for source in list_of_sources:
    #tempoary lists to store each element we want to extract
    list_title = []
    list_text = []
    list_source =[]

    for article_extract in source.articles:
        #appending the elements we want to extract
        list_title.append(article_extract.title)
        list_text.append(article_extract.text)
        list_source.append(article_extract.source_url)

    df_temp = pd.DataFrame({'Title': list_title, 'Text': list_text, 'Source': list_source})
    #Append to the final DataFrame
    df_articles = df_articles.append(df_temp, ignore_index = True)

#save to csv
df_articles.to_csv(scraped_articles_v2)
