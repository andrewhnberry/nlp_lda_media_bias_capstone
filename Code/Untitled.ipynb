{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "article extracted\n",
      "source extracted\n"
     ]
    }
   ],
   "source": [
    "# Scraping Articles using newspaper3k package: Multi-Threading Edition\n",
    "# Documentation could be found here: https://newspaper.readthedocs.io/en/latest/\n",
    "\n",
    "# Import Newspaper API that I will be using\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import Source\n",
    "from newspaper import news_pool\n",
    "\n",
    "# Other important Plugins\n",
    "import pandas as pd\n",
    "\n",
    "#List of online sources we want to scrape\n",
    "cnn = newspaper.build(\"https://edition.cnn.com/\", memoize_articles=False)\n",
    "bbc = newspaper.build(\"https://www.bbc.com/news\",memoize_articles=False)\n",
    "slate = newspaper.build('https://slate.com', memoize_articles=False)\n",
    "breitbart = newspaper.build(\"https://www.breitbart.com/politics/\", memoize_articles=False)\n",
    "politico = newspaper.build(\"https://www.politico.com/\", memoize_articles=False)\n",
    "thehill = newspaper.build(\"https://thehill.com/\", memoize_articles=False)\n",
    "cbc = newspaper.build('https://www.cbc.ca/news', memoize_articles=False)\n",
    "washingtonpost = newspaper.build(\"https://www.washingtonpost.com/\", memoize_articles=False)\n",
    "globeandmail = newspaper.build(\"https://www.theglobeandmail.com/\", memoize_articles=False)\n",
    "tc = newspaper.build('https://techcrunch.com/', memoize_articles=False)\n",
    "gamespot = newspaper.build('https://www.gamespot.com/news/', memoize_articles=False)\n",
    "globalnewsca = newspaper.build('https://globalnews.ca/', memoize_articles=False)\n",
    "thestar = newspaper.build('https://www.thestar.com/', memoize_articles=False)\n",
    "cna = newspaper.build('https://www.channelnewsasia.com/news/international', memoize_articles=False)\n",
    "\n",
    "#Combine all the sources\n",
    "list_of_sources = [cnn,bbc,slate,breitbart,politico,\n",
    "                   thehill,cbc,washingtonpost,globeandmail,\n",
    "                   tc,gamespot,globalnewsca,thestar,cna]\n",
    "\n",
    "#Intaitate Muli-Threading Downloads\n",
    "#WARNING: keep the threads_per_source at a reasonable number\n",
    "news_pool.set(list_of_sources, threads_per_source = 3) #2 threads per each source\n",
    "news_pool.join()\n",
    "\n",
    "#Create our final dataframe\n",
    "df_articles = pd.DataFrame()\n",
    "\n",
    "#Create a download limit per sources\n",
    "limit = 10\n",
    "\n",
    "for source in list_of_sources:\n",
    "    #tempoary lists to store each element we want to extract\n",
    "    list_title = []\n",
    "    list_text = []\n",
    "    list_source =[]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for article_extract in source.articles:\n",
    "\n",
    "        if count > limit:\n",
    "            break\n",
    "\n",
    "        #appending the elements we want to extract\n",
    "        list_title.append(article_extract.title)\n",
    "        list_text.append(article_extract.text)\n",
    "        list_source.append(article_extract.source_url)\n",
    "\n",
    "        #Update count\n",
    "        count +=1\n",
    "        print(\"article extracted\")\n",
    "\n",
    "    df_temp = pd.DataFrame({'Title': list_title, 'Text': list_text, 'Source': list_source})\n",
    "    #Append to the final DataFrame\n",
    "    df_articles = df_articles.append(df_temp, ignore_index = True)\n",
    "    print('source extracted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Media</td>\n",
       "      <td></td>\n",
       "      <td>https://edition.cnn.com/africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td></td>\n",
       "      <td>https://edition.cnn.com/africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rugby</td>\n",
       "      <td></td>\n",
       "      <td>https://edition.cnn.com/africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>https://edition.cnn.com/uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>https://edition.cnn.com/uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>\\n                                            ...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.channelnewsasia.com/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>CNA Insider</td>\n",
       "      <td></td>\n",
       "      <td>https://www.channelnewsasia.com/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Singapore</td>\n",
       "      <td></td>\n",
       "      <td>https://www.channelnewsasia.com/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Asia</td>\n",
       "      <td></td>\n",
       "      <td>https://www.channelnewsasia.com/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>World</td>\n",
       "      <td></td>\n",
       "      <td>https://www.channelnewsasia.com/news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title Text  \\\n",
       "0                                                Media        \n",
       "1                                                 News        \n",
       "2                                                Rugby        \n",
       "3                                                 None        \n",
       "4                                                 None        \n",
       "..                                                 ...  ...   \n",
       "149  \\n                                            ...        \n",
       "150                                        CNA Insider        \n",
       "151                                          Singapore        \n",
       "152                                               Asia        \n",
       "153                                              World        \n",
       "\n",
       "                                   Source  \n",
       "0          https://edition.cnn.com/africa  \n",
       "1          https://edition.cnn.com/africa  \n",
       "2          https://edition.cnn.com/africa  \n",
       "3              https://edition.cnn.com/uk  \n",
       "4              https://edition.cnn.com/uk  \n",
       "..                                    ...  \n",
       "149  https://www.channelnewsasia.com/news  \n",
       "150  https://www.channelnewsasia.com/news  \n",
       "151  https://www.channelnewsasia.com/news  \n",
       "152  https://www.channelnewsasia.com/news  \n",
       "153  https://www.channelnewsasia.com/news  \n",
       "\n",
       "[154 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
